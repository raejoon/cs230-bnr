import numpy as np
import keras.models as models
import keras.layers as layers

def load_asts_from_dataset(ast_dirpath):
    pass

def load_asts_from_file(ast_filepath):
    """ Load asts from file.
    Note that npy file generated by Ben's code has the following shape.
    (num_timesteps, num_blocks, num_asts)
    
    Returns:
    all_matrix (np.array): (num_asts, num_timesteps, num_blocks) data set
    """
    orig_mat = np.load(ast_filepath)
    orig_mat = np.swapaxes(orig_mat, 0, 1)
    orig_mat = np.swapaxes(orig_mat, 0, 2)
    return orig_mat

def save_asts_to_file(X, ast_filepath):
    X = np.swapaxes(X, 0, 2)
    X = np.swapaxes(X, 0, 1)  
    np.save(ast_filepath, X)


def get_output_labels(X):
    num_asts = X.shape[0]
    num_timesteps = X.shape[1]

    Y = np.zeros(X.shape)
    for t in range(num_timesteps - 1):
        Y[:,t,:] = X[:,t+1,:]

    Y[:,num_timesteps-1,-1] = np.ones(num_asts)
    return Y 


def create_model(X):
    """ Returns LSTM model for predicting next block in a given AST and a
    timestep
    """
    _, num_timestep, num_blocks = np.shape(X)
    hidden_size = 128
    dropout_p = 0.5
    
    model = models.Sequential()
    
    #Add LSTM layer with 128 hidden units, tanh nonlinearity
    model.add(layers.LSTM(hidden_size, 
                          activation='tanh',
                          return_sequences=True,
                          input_shape=(num_timestep, num_blocks)))
    
    #Add Dropout
    #What about rescalling?, we shouuld add scale up 
    #to avoid modifying it during test time
    model.add(layers.Dropout(dropout_p))
    
    #Add Dense layer
    model.add(layers.Dense(num_blocks,
                           activation='softmax'))
    
    #Configure the learning process
    model.compile(loss="categorical_crossentropy",
                  optimizer="adam",
                  metrics=["accuracy"])
    
    model.summary()
    return model

def fit_model(model, X, Y):
    history = model.fit(X, Y, 
                        validation_split=0.1, 
                        epochs=50, 
                        batch_size=16, 
                        verbose=1)
    return history

def get_embeddings(model, X):
    pass

def save_embeddings(embed_dict, filepath):
    pass

def load_embeddings(filepath):
    pass

