import numpy as np
import keras
import tensorflow as tf
from keras.utils import to_categorical
import keras.models as models
import keras.layers as layers
import utils

np.random.seed(1)
tf.set_random_seed(1)

class History(keras.callbacks.Callback):
    def __init__(self, X_train, Y_train):
        super().__init__()
        self.X_train = X_train
        self.Y_train = Y_train
        self.effective_accuracy = {"train": [], "validate": []}

    def _get_accuracy(self, X, Y_true, Y_pred):
        Y_true_class = np.argmax(Y_true, axis=-1)
        Y_pred_class = np.argmax(Y_pred, axis=-1)
        accuracy = np.sum(Y_true_class == Y_pred_class) / Y_true_class.size
        return accuracy
    
    def _get_effective_accuracy(self, X, Y_true, Y_pred):
        num_blocks = np.shape(X)[2]
        
        X_class = np.argmax(X, axis=-1).flatten()
        Y_true_class = np.argmax(Y_true, axis=-1).flatten()
        Y_pred_class = np.argmax(Y_pred, axis=-1).flatten()
        
        Y_true_eff = Y_true_class[X_class != num_blocks - 1]
        Y_pred_eff = Y_pred_class[X_class != num_blocks - 1] 
        accuracy = np.sum(Y_true_eff == Y_pred_eff) / Y_true_eff.size
        return accuracy
    
    def on_epoch_end(self, epoch, logs={}):
        X = self.X_train
        Y_true = self.Y_train
        Y_pred = self.model.predict(X)

        self.effective_accuracy["train"].append(
            self._get_effective_accuracy(X, Y_true, Y_pred))

        X = self.validation_data[0]
        Y_true = self.validation_data[1]
        Y_pred = self.model.predict(X)

        self.effective_accuracy["validate"].append(
            self._get_effective_accuracy(X, Y_true, Y_pred))


def load_asts_from_dataset(ast_dirpath):
    pass


def load_asts_from_file(ast_filepath, raejoon=False):
    """ Load asts from file.
    Note that npy file generated by Ben's code has the following shape.
    (num_timesteps, num_blocks, num_asts)
    
    Returns:
    all_matrix (np.array): (num_asts, num_timesteps, num_blocks) data set
    """
    if not raejoon:
        orig_mat = np.load(ast_filepath)
        orig_mat = np.swapaxes(orig_mat, 0, 1)
        orig_mat = np.swapaxes(orig_mat, 0, 2)
        return orig_mat
    else:
        return to_categorical(np.load(ast_filepath))

def save_asts_to_file(X, ast_filepath):
    X = np.swapaxes(X, 0, 2)
    X = np.swapaxes(X, 0, 1)  
    np.save(ast_filepath, X)


def get_output_labels(X):
    num_asts = X.shape[0]
    num_timesteps = X.shape[1]

    Y = np.zeros(X.shape)
    for t in range(num_timesteps - 1):
        Y[:,t,:] = X[:,t+1,:]

    Y[:,num_timesteps-1,-1] = np.ones(num_asts)
    return Y 


def create_model(X):
    """ Returns LSTM model for predicting next block in a given AST and a
    timestep
    """
    _, num_timestep, num_blocks = np.shape(X)
    hidden_size = 1
    dropout_p = 0.5
    
    model = models.Sequential()
    
    #Add LSTM layer with 128 hidden units, tanh nonlinearity
    model.add(layers.LSTM(hidden_size, 
                          activation='tanh',
                          return_sequences=True,
                          input_shape=(num_timestep, num_blocks)))
    
    #Add Dropout
    #What about rescalling?, we shouuld add scale up 
    #to avoid modifying it during test time
    model.add(layers.Dropout(dropout_p))
    
    #Add Dense layer
    model.add(layers.Dense(num_blocks,
                           activation='softmax'))
    
    #Configure the learning process
    model.compile(loss="categorical_crossentropy",
                  optimizer="adam",
                  metrics=["accuracy"])
    
    model.summary()
    return model


def fit_model(model, X, Y, epochs=50):
    split_ind = int((1 - 0.1) * np.shape(X)[0])
    X_train = X[np.arange(split_ind), :, :]
    Y_train = Y[np.arange(split_ind), :, :]
    X_validate = X[np.arange(split_ind, np.shape(X)[0]), :, :]
    Y_validate = Y[np.arange(split_ind, np.shape(Y)[0]), :, :]

    custom_history = History(X_train, Y_train)

    history = model.fit(X_train, Y_train, 
                        validation_data=(X_validate, Y_validate), 
                        epochs=epochs, 
                        batch_size=16, 
                        verbose=1, callbacks=[custom_history])
    return custom_history


def _get_input_with_dummy_asts(sorted_X, ast_dirpath):
    existing_ids = utils.get_ast_ids(ast_dirpath)
    ast_range_size = max(existing_ids) + 1
    # add dummy row for Keras
    filled_X = np.zeros((ast_range_size + 1, 
                         np.shape(sorted_X)[1], np.shape(sorted_X)[2]))

    # let 0 row to be dummy, thanks to Keras
    for ind, ast_id in enumerate(existing_ids):
        filled_X[ast_id + 1, :, :] = sorted_X[ind, :, :]

    return filled_X


def get_embeddings(model, X, ast_count_filepath):
    filled_X = _get_input_with_dummy_asts(X, ast_count_filepath)

    from keras import backend as K
    get_lstm_layer_output = K.function([model.layers[0].input],
                                       [model.layers[0].output])
    
    lstm_output = get_lstm_layer_output([filled_X])[0]
    lstm_shape = np.shape(lstm_output)
    lstm_output = np.reshape(lstm_output, (lstm_shape[0], lstm_shape[1]))
    assert(len(np.shape(lstm_output)) == 2)
    return lstm_output

def save_embeddings(embed_dict, filepath):
    np.save(filepath, embed_dict)
    print("Saved embeddings at " + filepath)

def load_embeddings(filepath):
    embed_dict = np.load(filepath)
    print("Loaded embeddings from " + filepath)
    return embed_dict

